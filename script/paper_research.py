#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è®ºæ–‡æ£€ç´¢åŠ©æ‰‹ - ä¸»è„šæœ¬

åŠŸèƒ½ï¼š
1. ä»æŸ¥è¯¢æ–‡ä»¶è¯»å–æœç´¢æŸ¥è¯¢
2. ä½¿ç”¨ findpapers è¿›è¡Œå¤šæ•°æ®åº“æ£€ç´¢
3. ä¸‹è½½è®ºæ–‡ PDF
4. ä¿å­˜æ£€ç´¢ç»“æœï¼ˆJSON + PDFï¼‰

ä½œè€…ï¼šNova & ä»²æ¸…
åˆ›å»ºæ—¶é—´ï¼š2026-02-22
æ›´æ–°æ—¶é—´ï¼š2026-02-22ï¼ˆç§»é™¤é‚®ä»¶é€šçŸ¥ï¼‰
"""

import sys
import os
import json
import argparse
import subprocess
from datetime import datetime
from pathlib import Path

# ç¡®ä¿ä½¿ç”¨ UTF-8 ç¼–ç 
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

try:
    from findpapers import search as findpapers_search
    import arxiv
except ImportError as e:
    print(f"é”™è¯¯ï¼šå¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–ï¼špip install findpapers arxiv")
    sys.exit(1)


class PaperResearchAssistant:
    """è®ºæ–‡æ£€ç´¢åŠ©æ‰‹ä¸»ç±»"""

    def __init__(self, query_file=None, query=None, output_dir=None, limit=10, limit_per_database=None):
        """
        åˆå§‹åŒ–åŠ©æ‰‹

        å‚æ•°:
            query_file: æŸ¥è¯¢æ–‡ä»¶è·¯å¾„
            query: ç›´æ¥æŸ¥è¯¢å­—ç¬¦ä¸²ï¼ˆä¼˜å…ˆçº§é«˜äº query_fileï¼‰
            output_dir: è¾“å‡ºç›®å½•
            limit: æ£€ç´¢è®ºæ–‡æ•°é‡é™åˆ¶ï¼ˆæ€»æ•°é‡ï¼‰
            limit_per_database: æ¯ä¸ªæ•°æ®åº“çš„æ£€ç´¢æ•°é‡é™åˆ¶ï¼ˆNone = æ— é™åˆ¶ï¼‰
        """
        # DEBUG MARKER: v3 - added limit_per_database
        import sys
        print(f"[DEBUG] PaperResearchAssistant.__init__ called with limit_per_database", file=sys.stderr)
        self.query = None
        self.output_dir = Path(output_dir) if output_dir else None
        self.limit = limit
        self.limit_per_database = limit_per_database
        self.results = None

        # è¯»å–æŸ¥è¯¢
        if query:
            self.query = query.strip()
        elif query_file:
            self.query = self._read_query_file(query_file)
        else:
            raise ValueError("å¿…é¡»æä¾› query æˆ– query_file å‚æ•°")

        if not self.query:
            raise ValueError("æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º")

        # è®¾ç½®è¾“å‡ºç›®å½•
        if not self.output_dir:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            self.output_dir = PROJECT_ROOT / "papers" / f"search_{timestamp}"

        self.output_dir.mkdir(parents=True, exist_ok=True)

        print(f"="*60)
        print(f"è®ºæ–‡æ£€ç´¢åŠ©æ‰‹")
        print(f"="*60)
        print(f"æŸ¥è¯¢: {self.query}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"æ•°é‡é™åˆ¶: {self.limit}")
        print(f"="*60)

    def _read_query_file(self, query_file):
        """è¯»å–æŸ¥è¯¢æ–‡ä»¶"""
        query_path = PROJECT_ROOT / "queries" / query_file
        if not query_path.exists():
            query_path = Path(query_file)

        if not query_path.exists():
            raise FileNotFoundError(f"æŸ¥è¯¢æ–‡ä»¶ä¸å­˜åœ¨: {query_file}")

        with open(query_path, 'r', encoding='utf-8') as f:
            query = f.read().strip()

        print(f"ä»æ–‡ä»¶è¯»å–æŸ¥è¯¢: {query_path}")
        return query

    def search_papers(self):
        """ä½¿ç”¨ findpapers æ£€ç´¢è®ºæ–‡"""
        print("\n[1/3] å¼€å§‹æ£€ç´¢è®ºæ–‡...")
        print(f"æ•°æ®åº“: arXiv, PubMed, ACM (å…è´¹)")
        print(f"æŸ¥è¯¢æ ¼å¼: {self.query}")

        try:
            # è¾“å‡ºæ–‡ä»¶è·¯å¾„
            output_file = str(self.output_dir / "results.json")

            # è°ƒç”¨ findpapers
            findpapers_search(
                query=self.query,
                outputpath=output_file,
                limit=self.limit,
                limit_per_database=self.limit_per_database,
                # å¯ä»¥æŒ‡å®šæ•°æ®åº“
                # databases=['arxiv', 'pubmed'],
            )

            # è¯»å–ç»“æœ
            with open(output_file, 'r', encoding='utf-8') as f:
                self.results = json.load(f)

            num_papers = self.results.get('number_of_papers', 0)
            print(f"\næ£€ç´¢å®Œæˆï¼æ‰¾åˆ° {num_papers} ç¯‡è®ºæ–‡")
            print(f"ç»“æœä¿å­˜è‡³: {output_file}")

            return True

        except Exception as e:
            print(f"\né”™è¯¯ï¼šæ£€ç´¢å¤±è´¥ - {e}")
            return False

    def download_pdfs(self):
        """ä¸‹è½½è®ºæ–‡ PDF"""
        if not self.results:
            print("é”™è¯¯ï¼šæ²¡æœ‰æ£€ç´¢ç»“æœï¼Œè¯·å…ˆè¿è¡Œ search_papers()")
            return False

        print(f"\n[2/3] å¼€å§‹ä¸‹è½½ PDF...")

        papers = self.results.get('papers', [])
        downloaded = 0
        failed = 0

        # åˆ›å»º PDF ä¿å­˜ç›®å½•
        pdf_dir = self.output_dir / "pdfs"
        pdf_dir.mkdir(exist_ok=True)

        for i, paper in enumerate(papers, 1):
            title = paper.get('title', 'Unknown')
            print(f"\n[{i}/{len(papers)}] {title}")

            # å°è¯•ä»ä¸åŒæ¥æºä¸‹è½½
            pdf_url = None
            filename = None

            # 1. æ£€æŸ¥ arXiv URL
            for url in paper.get('urls', []):
                if 'arxiv.org' in url:
                    arxiv_id = url.split('/')[-1]
                    try:
                        # ä½¿ç”¨ arxiv åº“ä¸‹è½½
                        search = arxiv.Search(id_list=[arxiv_id])
                        for result in search.results():
                            pdf_filename = pdf_dir / f"{arxiv_id}.pdf"
                            result.download_pdf(filename=str(pdf_filename))
                            print(f"  âœ“ å·²ä¸‹è½½: {pdf_filename.name}")
                            downloaded += 1
                            break
                    except Exception as e:
                        print(f"  âœ— arXiv ä¸‹è½½å¤±è´¥: {e}")
                        failed += 1
                    break
            else:
                # é arXiv è®ºæ–‡ï¼Œå°è¯•ç›´æ¥ URL ä¸‹è½½
                for url in paper.get('urls', []):
                    if '.pdf' in url:
                        print(f"  âš  PDF URL: {url}")
                        print(f"  â„¹ è¯·æ‰‹åŠ¨ä¸‹è½½ï¼ˆéœ€è¦æœºæ„æƒé™æˆ–è®¢é˜…ï¼‰")
                        break
                else:
                    print(f"  âš  æœªæ‰¾åˆ° PDF ä¸‹è½½é“¾æ¥")

        print(f"\nä¸‹è½½å®Œæˆï¼")
        print(f"  æˆåŠŸ: {downloaded} ç¯‡")
        print(f"  å¤±è´¥/éœ€æ‰‹åŠ¨: {failed} ç¯‡")

        return downloaded > 0

    def generate_summary(self):
        """ç”Ÿæˆæ£€ç´¢ç»“æœæ‘˜è¦"""
        if not self.results:
            return None

        papers = self.results.get('papers', [])
        summary = {
            'query': self.query,
            'total': len(papers),
            'by_database': self.results.get('number_of_papers_by_database', {}),
            'papers': []
        }

        for paper in papers[:10]:  # åªæ˜¾ç¤ºå‰ 10 ç¯‡
            summary['papers'].append({
                'title': paper.get('title', ''),
                'authors': paper.get('authors', [])[:3],  # åªæ˜¾ç¤ºå‰ 3 ä¸ªä½œè€…
                'year': paper.get('publication_date', '')[:4] if paper.get('publication_date') else '',
                'database': paper.get('databases', ['Unknown'])[0] if paper.get('databases') else 'Unknown'
            })

        return summary

    def show_completion_summary(self):
        """æ˜¾ç¤ºå®Œæˆæ‘˜è¦å’Œæ–‡ä»¶ä½ç½®"""
        print(f"\n{'='*60}")
        print(f"âœ“ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
        print(f"{'='*60}")

        summary = self.generate_summary()

        print(f"\nğŸ“Š æ£€ç´¢ç»Ÿè®¡")
        print(f"{'â”€'*40}")
        print(f"  æ€»æ•°: {summary['total']} ç¯‡")

        for db, count in summary['by_database'].items():
            print(f"  {db}: {count} ç¯‡")

        print(f"\nğŸ“ æ–‡ä»¶ä½ç½®")
        print(f"{'â”€'*40}")
        print(f"  ç»“æœç›®å½•: {self.output_dir}")
        print(f"  JSONæ–‡ä»¶: {self.output_dir / 'results.json'}")
        print(f"  PDFç›®å½•: {self.output_dir / 'pdfs'}")

        print(f"\nğŸ’¡ æ‰“å¼€æ–‡ä»¶å¤¹")
        print(f"{'â”€'*40}")
        print(f"  æ–‡ä»¶èµ„æºç®¡ç†å™¨ä¸­æ‰“å¼€:")
        print(f"  {self.output_dir}")

        # å°è¯•è‡ªåŠ¨æ‰“å¼€æ–‡ä»¶å¤¹
        try:
            if sys.platform == 'win32':
                os.startfile(self.output_dir)
            elif sys.platform == 'darwin':
                subprocess.run(['open', str(self.output_dir)])
            else:
                subprocess.run(['xdg-open', str(self.output_dir)])
            print(f"\n  âœ“ å·²è‡ªåŠ¨æ‰“å¼€æ–‡ä»¶å¤¹")
        except Exception as e:
            print(f"\n  â„¹ å¦‚éœ€æ‰‹åŠ¨æ‰“å¼€ï¼Œå¤åˆ¶ä¸Šè¿°è·¯å¾„")

        print(f"\n{'='*60}\n")

    def run(self, open_folder=True):
        """è¿è¡Œå®Œæ•´æµç¨‹

        å‚æ•°:
            open_folder: æ˜¯å¦è‡ªåŠ¨æ‰“å¼€ç»“æœæ–‡ä»¶å¤¹
        """
        print(f"\nå¼€å§‹æ‰§è¡Œæ£€ç´¢æµç¨‹...")
        print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        try:
            # 1. æ£€ç´¢
            if not self.search_papers():
                return False

            # 2. ä¸‹è½½ PDF
            self.download_pdfs()

            # 3. æ˜¾ç¤ºå®Œæˆæ‘˜è¦
            self.show_completion_summary()

            return True

        except KeyboardInterrupt:
            print(f"\n\nä¸­æ–­ï¼šç”¨æˆ·å–æ¶ˆæ“ä½œ")
            return False
        except Exception as e:
            print(f"\n\né”™è¯¯ï¼š{e}")
            import traceback
            traceback.print_exc()
            return False


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description='è®ºæ–‡æ£€ç´¢åŠ©æ‰‹ - å¤šæ•°æ®åº“è®ºæ–‡æ£€ç´¢å’Œä¸‹è½½å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä»æŸ¥è¯¢æ–‡ä»¶æ£€ç´¢
  python paper_research.py -q test_query.txt

  # ç›´æ¥æŒ‡å®šæŸ¥è¯¢ï¼ˆæ³¨æ„æ ¼å¼ï¼‰
  python paper_research.py --query '[\"AI\"] AND [\"healthcare\"]'

  # æŒ‡å®šè¾“å‡ºç›®å½•å’Œæ•°é‡
  python paper_research.py -q test_query.txt -o papers/my_search -l 20

  # ç¦ç”¨é‚®ä»¶é€šçŸ¥
  python paper_research.py -q test_query.txt --no-notify

æŸ¥è¯¢æ ¼å¼:
  - æœ¯è¯­ç”¨æ–¹æ‹¬å·: [\"machine learning\"]
  - å¸ƒå°”è¿ç®—ç¬¦å¤§å†™: AND, OR, AND NOT
  - ç¤ºä¾‹: [\"AI\"] AND [\"deep learning\"] OR [\"neural networks\"]
        """
    )

    parser.add_argument(
        '-q', '--query-file',
        help='æŸ¥è¯¢æ–‡ä»¶è·¯å¾„ï¼ˆä¿å­˜åœ¨ queries/ ç›®å½•ä¸‹ï¼‰'
    )

    parser.add_argument(
        '--query',
        help='ç›´æ¥æŒ‡å®šæŸ¥è¯¢å­—ç¬¦ä¸²'
    )

    parser.add_argument(
        '-o', '--output-dir',
        help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼špapers/search_TIMESTAMPï¼‰'
    )

    parser.add_argument(
        '-l', '--limit',
        type=int,
        default=10,
        help='æ£€ç´¢è®ºæ–‡æ•°é‡é™åˆ¶ï¼ˆé»˜è®¤ï¼š10ï¼‰'
    )

    parser.add_argument(
        '--limit-per-database',
        type=int,
        default=None,
        help='æ¯ä¸ªæ•°æ®åº“çš„æ£€ç´¢æ•°é‡é™åˆ¶ï¼ˆé»˜è®¤ï¼šæ— é™åˆ¶ï¼Œå¯ä»å¤šä¸ªæ•°æ®åº“è·å–æ›´å¤šç»“æœï¼‰'
    )

    parser.add_argument(
        '--no-notify',
        action='store_true',
        help='ç¦ç”¨é‚®ä»¶é€šçŸ¥'
    )

    args = parser.parse_args()

    # éªŒè¯å‚æ•°
    if not args.query_file and not args.query:
        parser.print_help()
        print("\né”™è¯¯ï¼šå¿…é¡»æä¾› -q/--query-file æˆ– --query å‚æ•°")
        sys.exit(1)

    try:
        # åˆ›å»ºåŠ©æ‰‹å¹¶è¿è¡Œ
        assistant = PaperResearchAssistant(
            query_file=args.query_file,
            query=args.query,
            output_dir=args.output_dir,
            limit=args.limit,
            limit_per_database=args.limit_per_database
        )

        success = assistant.run()
        sys.exit(0 if success else 1)

    except Exception as e:
        print(f"\né”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
